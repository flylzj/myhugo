---
title: "天天基金网爬虫"
date: 2020-01-12T12:25:43+08:00
draft: false
---

## 1. 需求分析

我们的需求是爬取所有基金的股票持仓数据。这个数据是访问类似[`http://fundf10.eastmoney.com/ccmx_006177.html`](http://fundf10.eastmoney.com/ccmx_006177.html)这种链接看到的。

不难看出这个链接的主要是根据那串基金代码不同来返回不同数据的（这里是006177），那我们只要拿到所有基金的代码然后来一次来访问这个页面就能拿到所有基金的股票持仓了。


##### 1.1 请求网页

首先我们先访问查看所有基金的页面，打开chrome访问[`http://fund.eastmoney.com/data/fundranking.html#tall;c0;r;szzf;pn50;ddesc;qsd20190112;qed20200112;qdii;zq;gg;gzbd;gzfs;bbzt;sfbb`](http://fund.eastmoney.com/data/fundranking.html#tall;c0;r;szzf;pn50;ddesc;qsd20190112;qed20200112;qdii;zq;gg;gzbd;gzfs;bbzt;sfbb)

等待网页加载完成后

按`f12`进入开发者模式

![1](1.png)

然后按`f5`刷新页面，点击`network`查看网络请求，这里可以看到它全部的网络请求

![2](2.png)

##### 1.2 寻找数据来源

这时候network里还有很多请求，我们怎么才能找到我们需要的数据呢。

因为我们需要的是左边表格的内容，只需要随便复制一点表格的数据，然后到左边的开发者工具寻找就好了。步骤如下：

![3](3.png)

这里找到了两个数据源，点击去分别看下他们的数据。

这个链接[`http://fund.eastmoney.com/js/fundcode_search.js?v=20200112122652`](http://fund.eastmoney.com/js/fundcode_search.js?v=20200112122652)里面数据是这样的

![4](4.png)

而这个[`http://fund.eastmoney.com/data/rankhandler.aspx?op=ph&dt=kf&ft=all&rs=&gs=0&sc=zzf&st=desc&sd=2019-01-12&ed=2020-01-12&qdii=&tabSubtype=,,,,,&pi=1&pn=50&dx=1&v=0.7898034154393669`](http://fund.eastmoney.com/data/rankhandler.aspx?op=ph&dt=kf&ft=all&rs=&gs=0&sc=zzf&st=desc&sd=2019-01-12&ed=2020-01-12&qdii=&tabSubtype=,,,,,&pi=1&pn=50&dx=1&v=0.7898034154393669)里面是这样的

![5](5.png)

很明显这个带了很多百分号的第二个网页，那我们这就算找到了它请求基金数据的url了。

这里的链接只返回了一页的数据也就是五十条，接下来我们要看看怎么找全部的数据。

点击页面里面的下一页，这时候network里面会多一个请求。

![6](6.png)

仔细看看比较一下他们请求参数的区别

![7](7.png)

![8](8.png)

现在应该能猜到pi是控制页数的吧，其它的参数可以不管。我们只要改变页码就好了。

##### 小技巧

正常的流程应该是上面这样的，但是我们看到有一个pn参数，值是50，然后它每页刚刚是五十条数据，
我们可以尝试一下改一改这个值，让他多返回一点，这样的好处是一次返回的数据多请求的数量减少，可以加快爬取速度，减少了被反爬检测的风险。

我们可以右键这个请求然后复制他的链接（如图）

![9](9.png)

然后把他复制到浏览器新标签页的地址栏，把`pn=50`改成`pn=5000`。

这里如果是正常的程序员应该限制一次返回的最大数据才对，可是它居然真的5000条全都返回了，那这样的我们就可以通过一次请求就拿到所有的基金代码了。


##### 1.3 流程梳理

整个项目代码的编写流程可以分为：

* 爬取基金代码数据
* 通过基金代码获取股票持仓数据
* 清洗数据
* 将数据写入数据库

## 2. 环境准备

现在开始搭建开发环境。

我们用到的软件有

* python3
* pycharm（专业版和社区版都行）
* mysql（用于存储爬取到的数据）
* navicate premium（可选，可以用来可视化mysql。）

安装过程我就不详细写了，都可以百度的到

##### 2.1 新建pycharm项目

打开pycharm点`Create New Project`

![11](11.png)

`location`的话随意，解释器选`New environment using`，这样可以在当前目录新建一个虚拟环境，一个项目一个虚拟环境比较好管理。

![12](12.png)

点create等他创建虚拟环境，现在是一个带虚拟环境的空项目。

![13](13.png)

##### 2.2 mysql数据库创建

首先保证mysql已经安装而且服务正在运行，
打开navicate，新建一个连接

![14](14.png)

填好配置点确定就好了。

然后新建数据库

![15](15.png)

填写数据库名并填写字符集，点确定数据库就建好了，建完我们就多了一个数据库了。

![16](16.png)

![17](17.png)

建表的过程我们放到代码里，这里就不建表了。

## 3. 代码编写

##### 3.1 整理需要的第三方库

我们需要用到的库有

* requests(用于发起http请求)
* sqlalchemy(orm框架，可以把数据库中表抽象成对象)
* pymysql(用于连接数据库)

##### 3.2 编写代码架构

## 4. 测试运行

## 5. 项目打包&&部署



